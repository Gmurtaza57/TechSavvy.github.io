<!DOCTYPE HTML>
<html>
<head>
    <title>Ghulam Murtaza: Data Engineering Portfolio</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
    <script src="https://www.gstatic.com/firebasejs/8.6.8/firebase-app.js"></script>
<script src="https://www.gstatic.com/firebasejs/8.6.8/firebase-database.js"></script>
</head>
<body class="is-preload">



    <!-- Intro -->
    <div id="intro">
        <h1>Ghulam Murtaza Portfolio</h1>
        <p>Explore a portfolio rich with projects in data scraping and data cleaning using SQL. Connect with me on <a href="https://www.linkedin.com/in/ghulam-murtaza-ms-310741110/">@My_Linkedin</a>.</p>
        <ul class="actions">
            <li><a href="#header" class="button icon solid solo fa-arrow-down scrolly">Continue</a></li>
        </ul>
    </div>

    <!-- Header -->
    <header id="header">
        <a href="index.html" class="logo">Murtaza: ğŸ“ˆData Engineer</a>
    </header>

    <!-- Main -->
    <main id="main">
        <!-- Posts -->
        <section class="posts">
            <article>
             <header>
            <h2><a href="https://your_project_link_here.com">Retail Sales ETL Pipeline- Snowflake to BigQuery</a></h2>
            </header>
        <a href="#" class="image fit"><img src="images/workflow.png" alt="Retail Sales Visualization Image" /></a>
        <div class="project-section">
            <p>ğŸŒŸ <strong>Highlights:</strong> Retail Sales Data Transformation and Visualization</p>
    
            <p><strong>Skills:</strong> Snowflake Â· Alteryx Â· BigQuery Â· Tableau Â· ODBC Â· SQL Â· Windows Task Scheduler Â· Data Warehousing Â· ETL</p>
    
            <p class="center-block"><strong>                   ğŸ“¦ Data Extraction:</strong><p>
    
            <p>â€¢      Utilized UC Irvine Retail Sales Data with 1,067,372 records. Dataset Link: <a href="https://lnkd.in/ecSReh7q" target="_blank">here</a>.</p>
            <p>â€¢      Excel to CSV conversion.</p>
            <p>â€¢      Established warehouse, database, schema, and table in Snowflake for raw data.</p>
            <p>â€¢      Verified data integration on Snowflake with SQL Queries: <a href="https://github.com/Gmurtaza57/Retail_ETL_Pipeline/blob/main/Retail_Table_Queries_Snowflake.sql" target="_blank">Link for Code</a></p>
    
            <p class="center-block"><strong>                   ğŸ”„ Transformation:</strong></p>
    
            <p>â€¢     Snowflake to Alteryx connection via ODBC.</p>
            <p>â€¢     Alteryx Workflow: Null value handling (customerID set to -1, description set to "unknown").</p>
            <p>â€¢     80% threshold fuzzy match for description discrepancies.</p>
            <p>â€¢     Deduplication based on unique values.</p>
    
            <p class="center-block"><strong>                  ğŸ”§ Feature Engineering:</strong><p>
    
            <p>â€¢     New columns for Day, Hour, Month for visualizations.</p>
            <p>â€¢     New columns for total spend and total return {Price*Quantity}.</p>
            <button class="read-more-btn">Read More</button>
            <div class="project-details hidden">
            <p class="center-block"><strong>                   ğŸ’¾ Loading:</strong> </p>
    
            <p>â€¢     Data transferred to BigQuery using ODBC.</p>
            <p>â€¢     Updated BigQuery Table only if new records found using primary keys: invoice, customerID, stock code for matching.</p>
            <p>â€¢     Validated data ingestion with SQL queries in BigQuery. <a href="https://github.com/Gmurtaza57/Retail_ETL_Pipeline/blob/main/BigQuery_Retail_Verification.sql" target="_blank">Link for Code</a></p>
    
            <p class="center-block"><strong>                   â² Automation:</strong></p>
    
            <p>â€¢     Used Windows Task Scheduler & batch script to run my workflow to update database every Sunday at 3 a.m.</p>
            <p>â€¢     Cost-effective alternative to paying for Alteryx cloud.</p>
    
            <p class="center-block"><strong>                  ğŸ“Š Visualization with Tableau:</strong></p>
    
            <p>â€¢     Established a live connection between Tableau and BigQuery.</p>
            <p>â€¢     The setup ensures that as data in BigQuery updates when my workflow is run on Sunday at 3 a.m, visualizations in Tableau reflect the changes instantly.</p>
    
            <p class="center-block"><strong>                   Charts created:</strong></p>
    
            <p>ğŸ“ˆ    Monthly sales & returns with dynamic filters.</p>
            <p>â€¢     Monthly breakdown that showcases both sales and returns with dynamic filter mechanism, for 'sales' and 'returns'.</p>
    
            <p>ğŸ”    Top 5 products sold in Oct-Nov.</p>
            <p>â€¢     Leveraging the peak sales months of October and November, visualization was crafted to spotlight the top 5 products that experienced the highest sales.</p>
    
            <p>ğŸŒ    Country Wise sales distribution.</p>
            <p>â°    Peak sales hours pie chart.</p>
            <p>ğŸ“†    Day-wise sales.</p>
            </div>

            <ul class="actions special">
            <li><a href="https://drive.google.com/file/d/1CJcaUyj5Hxe3wDrMoR5qjtcdTq3twp5d/view" class="button">View Workflow</a></li>
            <li><a href="https://public.tableau.com/app/profile/ghulam.murtaza6680/viz/Retail_Sales_Analysis_16968579680730/Dashboard1" class="button">View Dashboard</a></li>
            </ul>
            <div class="thumbs-up" data-id="like101" onclick="incrementCounter(this)">&#128077;</div>
            <div class="counter" data-id="like101">0</div>
            </article>
    <article>
    <header>
        <h2><a href="https://github.com/Gmurtaza57/reddit_pipeline">AWS-Based ETL Pipeline for Reddit Data</a></h2>
    </header>
    <a href="#" class="image fit"><img src="images/reddit_pipeline.jpg" alt="Reddit Data Pipeline Visualization Image" /></a>
    <div class="project-section">
        <p>ğŸ” <strong>Project Overview:</strong> Crafted an ETL pipeline for Reddit data, specifically targeting the subreddit "MachineLearning".</p>
        <p><strong>Tools & Platforms Used:</strong> Apache Airflow, Python, AWS S3, AWS Redshift, AWS Athena, AWS Glue, Tableau, PostgreSQL, Redis, Docker, PRAW, Pandas, Spark.</p>

        <p class="center-block"><strong>ğŸ“¦ Data Extraction:</strong></p>
        <p>â€¢ Connected to Reddit using PRAW with credentials stored securely.<br>
        â€¢ Extracted top posts from the "MachineLearning" subreddit using filters for the time period and post limit of 100.</p>

            <p class="center-block"><strong>ğŸ”„ Data Transformation:</strong></p>
            <p>â€¢ Employed Pandas and NumPy to handle data cleaning tasks like converting timestamps and handling boolean data types.<br>
            â€¢ Created a new 'tag_type' field from post titles.</p>

            <p class="center-block"><strong>ğŸ”§ AWS S3 Integration:</strong></p>
            <p>â€¢ Configured AWS S3 bucket for structured 'raw' data storage.<br>
            â€¢ Uploaded transformed data to S3 using S3FS.</p>
            <button class="read-more-btn">Read More</button>
            <div class="project-details hidden">
            <p class="center-block"><strong>â² Automation & Orchestration:</strong></p>
            <p>â€¢ Set up an Airflow DAG to manage the workflow and automated the ETL process to run daily at 12 a.m.<br>
            â€¢ Utilized Redis and PostgreSQL for message brokering and as a result backend.</p>

            <p class="center-block"><strong>ğŸ”„ Data Processing in AWS:</strong></p>
            <p>â€¢ AWS Glue detects new data and performs quality checks and data cleaning.</p>
             
            <p class="center-block"><strong>ğŸ’¾ AWS Transformation:</strong></p>
            <p>â€¢ An AWS Glue Crawler is scheduled to run at 2 AM daily for database and schema updates.</p>

            <p class="center-block"><strong>ğŸ” Data Validation and De-duplication:</strong></p>
            <p>â€¢ An additional pipeline checks for duplicates after the AWS Glue Crawler updates.</p>

            <p class="center-block"><strong>ğŸ“ˆ Visualization:</strong></p>
            <p>â€¢ Switched to Amazon Redshift for a live connection to update the Tableau dashboard with new S3 data.</p>

            <p class="center-block"><strong>ğŸ”— Version Control:</strong></p>
            <p>â€¢ GitHub is used for version control of the ETL pipeline logs and DAGs.</p>
        </div>
        <ul class="actions special">
            <li><a href="https://github.com/Gmurtaza57/reddit_pipeline" class="button">View Project</a></li>
            <li><a href="https://public.tableau.com/app/profile/ghulam.murtaza6680/viz/Reddit_pipeline_dashboard/Dashboard1" class="button">View Dashboard</a></li>
        </ul>
        <div class="thumbs-up" data-id="like121" onclick="incrementCounter(this)">&#128077;</div>
        <div class="counter" data-id="like121">0</div>
    </div>
</article>
        
    <article>
    <header>
        <h2><a href="https://github.com/Gmurtaza57/OTT_ETL_Pipeline">Azure-Based ETL Pipeline for OTT Platform Data</a></h2>
    </header>
    <a href="#" class="image fit"><img src="images/pipeline.png" alt="OTT Data Pipeline Visualization Image" /></a>
    <div class="project-section">
        <p>ğŸ” <strong>Project Overview:</strong> Developed an ETL pipeline for OTT platform data, focusing on data from popular platforms like Netflix, Disney+, and Prime.</p>
        <p><strong>Tools & Platforms Used:</strong> Azure, Azure Datalake Storage Gen2, Azure Data Factory, Azure Synapse Analytics, Azure SQL Database, Azure Logic App, PowerBI, Azure DevOps Git, Microsoft SSMS, Email Notifications.</p>
        
        <p class="center-block"><strong>ğŸ“¦ Data Extraction:</strong></p>
        <p>â€¢ Used a dummy dataset of OTT platforms from Kaggle.<br>
        â€¢ Set up a budget in Azure to receive email alerts when expenses cross a threshold.<br>
        â€¢ Initialized the Azure infrastructure with a resource group, Azure Datalake Storage Gen2, Azure Data Factory, Azure Synapse Analytics, and an Azure SQL Database.</p>

        <p class="center-block"><strong>ğŸ”„ Azure Data Factory Ingestion:</strong></p>
        <p>â€¢ Used a self-hosted runtime to transfer data from on-premise to Azure Data Lake Gen2.<br>
        â€¢ Implemented Azure Key Vault for data security and linked services for data transfer between on-premise and cloud.<br>
        â€¢ Developed pipelines to ingest data into a 'raw' folder in Azure Data Lake Gen2.<br>
        â€¢ Implemented two incremental data loading strategies:<br>
           a. Based on the 'last modified' timestamp of files.<br>
           b. Utilizing date details from filenames like "2022-04-12 110513.455313" for specific day data ingestion.</p>
        <button class="read-more-btn">Read More</button>
        <div class="project-details hidden">
        <p class="center-block"><strong>ğŸ”§ Efficiency Enhancements in Data Lake:</strong></p>
        <p>â€¢ Introduced a method to segregate incoming data into date-specific folders, rather than one large 'raw' folder.<br>
        â€¢ Leveraged this segregation in Azure Synapse to transform only the current day's data.</p>

        <p class="center-block"><strong>ğŸ”§ Transformation and Loading:</strong></p>
        <p>â€¢ Established a connection between Azure Datalake and Synapse Notebook, initializing a spark pool with three worker nodes for efficient code execution.<br>
        â€¢ Performed data cleaning by handling NULL values, removing duplicates, change column types and feature engineering.<br>
        â€¢ Saved the refined data in Datalake in <bold>Parquet Format</bold> and transferred it to the SQL database. Full code available <a href="https://github.com/Gmurtaza57/OTT_ETL_Pipeline/blob/main/Notebook%201.ipynb" target="_blank">here</a>.</p>

        <p class="center-block"><strong>â² Automation & Orchestration:</strong></p>
        <p>â€¢ Orchestrated all pipelines for extract, load, and transform.<br>
        â€¢ Set up triggers for daily execution at 12 pm.<br>
        â€¢ Integrated Azure Logic App to send email notifications for every stage of pipeline failures,</p>

        <p class="center-block"><strong>ğŸ“Š Visualization with PowerBI:</strong></p>
        <p>â€¢ Developed a PowerBI dashboard showcasing 4 charts.</p>

        <p class="center-block"><strong>ğŸš€ CI/CD Integration:</strong></p>
        <p>â€¢ Configured a CI/CD pipeline in Azure DevOps Git for the aforementioned pipeline.<br>
        â€¢ Added user Zaid Chaudhary to the DevOps Git project.<br>
        â€¢ He created a new branch in the DevOps Git project, designed a new pipeline, and sent pull requests for review<br>
        â€¢ Approved and merged the changes to the main branch of the project.</p>
    </div>
    <ul class="actions special">
        <li><a href="https://github.com/Gmurtaza57/OTT_ETL_Pipeline" class="button">View Project</a></li>
        <li><a href="https://drive.google.com/file/d/1NFWDjI0exh0iBmAMzKyKfTHgNXScyPjF/view?usp=sharing" class="button">View Dashboard</a></li>
    </ul>
            <div class="thumbs-up" data-id="like108" onclick="incrementCounter(this)">&#128077;</div>
            <div class="counter" data-id="like108">0</div>
</article>




            
            
            
            
            
            
            <article>
                <header>
                    <h2><a href="https://github.com/Gmurtaza57/Amazon-Web-Scrapping-Project">Amazon Data Scrapping</a></h2>
                </header>
                
                <a href="https://github.com/Gmurtaza57/Amazon-Web-Scrapping-Project" class="image fit"><img src="images/amazon.png" alt="" /></a>
                <div class="project-section">


                    <p><strong>Skills:</strong> Beautiful Soup Â· Simple Mail Transfer Protocol (SMTP)
                    </p>
                    <p>ğŸ” Developed a Python web scraping script for Amazon shirt price monitoring, leveraging BeautifulSoup and requests libraries.</p>
                    <p>ğŸ“§ Implemented an automated email system, notifying my Gmail for price drops below set threshold.</p>
                    <p>âš™ï¸ Utilized SMTP protocol & smtplib library, ensuring secure connection to Gmail's SMTP server for direct email delivery.</p>
                    <p>ğŸ—‚ï¸ Integrated CSV handling, recording shirt details & dates for insightful historical price tracking.</p>
                    <p>ğŸ’¡ Demonstrated expertise in data manipulation using datetime & csv libraries for accurate data management.</p>
                    <p>ğŸ’ª Overcame challenges in scraping data and handling Gmail's security measures.</p>
                  
                    
                </div>
                
                <ul class="actions special">
                    <li><a href="https://github.com/Gmurtaza57/Amazon-Web-Scrapping-Project" class="button">View Project</a></li>
                </ul>
                <div class="thumbs-up" data-id="like5" onclick="incrementCounter(this)">&#128077;</div>
                <div class="counter" data-id="like5">0</div>
            </article>
            <article>
                <header>
                    <h2><a href="https://github.com/Gmurtaza57/Twitter-Data-Scrapper-Project-">Twitter Data Scraper Using API</a></h2>
                </header>
                <a href="https://github.com/Gmurtaza57/Twitter-Data-Scrapper-Project-" class="image fit"><img src="images/twitter_data.png" alt="" /></a>
                <div class="project-section">

                    <p><strong>Skills:</strong> NLTK Â· twitterscraper Â· spaCy Â· pandas</p>
                    <p>ğŸ¦ Fetched and Analyzed Twitter Data of User 'realDonaldTrump' using twitterscraper.</p>
                    <p>ğŸ” Conducted Text Preprocessing: Tokenization, Stemming, Stop Word Removal, and Punctuation Handling using nltk.</p>
                    <p>ğŸ“Š Performed Exploratory Data Analysis (EDA) to Reveal Most Frequent Words in Tweets.</p>
                    <p>ğŸ“ˆ Visualized Top 20 Words with Bar Plot using seaborn Library.</p>
                    <p>ğŸ“ Utilized spacy for Named Entity Recognition (NER) to Extract Organizations and People Mentioned.</p>
                    <p>ğŸ¢ Presented Top 20 Organizations Mentioned with Bar Plot using seaborn.</p>
                    <p>ğŸ‘¥ Highlighted Top 20 People Mentioned with Bar Plot using seaborn.</p>
                    <p>ğŸ“Š Created Data Visualizations to Showcase Key Insights from Twitter Data.</p>
                    <p><br><strong>As of April 2023, free access to the API for academic or research purposes is unavailable. Third-party tools like TwitterScrapper can't retrieve tweets either. For those wanting to use Tweepy's official Twitter scraping API, there's a charge of $100 for searching tweets. As a result, the "try yourself" feature in my portfolio is not functioning.</strong></p>
                   
                    
                </div>
                
                <ul class="actions special">

                    <li><a href="https://github.com/Gmurtaza57/Twitter-Data-Scrapper-Project-" class="button">View Project</a></li>
                    <li><a href="twitter.html" class="button">Try Yourself</a></li>
                </ul>
                <div class="thumbs-up" data-id="like6" onclick="incrementCounter(this)">&#128077;</div>
                <div class="counter" data-id="like6">0</div>
            </article>

            <article>
    <header>
        <h2><a href="https://github.com/Gmurtaza57/Crypto_API_project">Automating Crypto Website API Pull Using Python</a></h2>
    </header>
    
    <a href="https://github.com/Gmurtaza57/Crypto_API_project" class="image fit"><img src="images/crypto_api.png" alt="" /></a>
    <div class="project-section">

        <p><strong>Skills:</strong> API Integration Â· JSON Handling Â· Pandas Â· Data Visualization Â· Seaborn Â· Error Handling
        </p>
        <p>ğŸ”„ Developed a Python script to automate the collection of cryptocurrency data from CoinMarketCap's API.</p>
        <p>ğŸ“Š Normalized JSON data into a Pandas dataframe, enabling efficient data analysis and transformation.</p>
        <p>ğŸ“ˆ Visualized cryptocurrency price percentage changes over different time frames using Seaborn.</p>
        <p>ğŸ• Added a timestamp column to track data retrieval time, aiding in trend analysis.</p>
        <p>ğŸ” Conducted focused analysis on specific cryptocurrencies, exemplified by Bitcoin price trends over time.</p>
        <p>ğŸ’¡ Demonstrated expertise in data manipulation, transformation, and visualization to generate actionable insights.</p>
        <p>ğŸ’ª Tackled challenges like API call errors, ensuring seamless data retrieval and analysis.</p>

    </div>
    
    <ul class="actions special">
        <li><a href="https://github.com/Gmurtaza57/Crypto_API_project" class="button">View Project</a></li>
    </ul>
                <div class="thumbs-up" data-id="like7" onclick="incrementCounter(this)">&#128077;</div>
                <div class="counter" data-id="like7">0</div>
</article>

        </section>
    </main>

    <!-- Footer -->
    <footer id="footer">
        <!-- You can add footer details here... -->
    </footer>
<style>
    p.center-block {
        width: 50%; /* or any desired width */
        margin-left: auto;
        margin-right: auto;
    }
</style>
    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
</body>
</html>
